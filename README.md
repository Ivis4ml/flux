# ðŸŒŠ Flux

**An Adaptive Post-Training Framework for Large Language Models**

> *"The best of all worlds"* â€” Synchronous stability + Asynchronous efficiency + Native simplicity

---

## Why Flux?

Existing RLHF frameworks force you to choose:

| Framework | Trade-off |
|-----------|-----------|
| **VERL** | Stable but slow (GPU bubbles from synchronous training) |
| **AReaL** | Fast but unstable (staleness from full async) |
| **Slime** | Simple but less flexible |

**Flux takes a different approach**: Instead of binary choices, we treat everything as a **continuous spectrum** that adapts during training.

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                                                                 â”‚
â”‚   Sync â—„â”€â”€â”€â”€â”€â”€â”€ Flux adapts in real-time â”€â”€â”€â”€â”€â”€â”€â–º Async        â”‚
â”‚                                                                 â”‚
â”‚   â€¢ Early training: More sync (stability)                       â”‚
â”‚   â€¢ Mid training: Balanced                                      â”‚
â”‚   â€¢ Late training: More async (speed)                           â”‚
â”‚                                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## Key Features

### ðŸŽ¯ Adaptive Async Control

Flux automatically adjusts the sync/async ratio based on training dynamics:

```python
# Flux maintains staleness within a target range
# Not too stale (unstable) and not too fresh (slow)
controller = AdaptiveAsyncController(
    target_staleness=0.15,  # Sweet spot
    tolerance=0.05
)
```

### âš¡ Native Performance

No Ray. No unnecessary abstraction layers. Direct integration with:

- **Megatron-LM** for training (TP, PP, DP, EP, CP)
- **SGLang** for inference (continuous batching, FP8)
- **CUDA IPC** for weight sync (zero-copy)

### ðŸ§  Smart Batching

Flux optimizes every batch:

- **Length-aware packing**: Minimize padding waste
- **Staleness balancing**: Reduce importance weight variance
- **Curriculum ordering**: Easy â†’ Hard as training progresses

### ðŸ”„ APRIL Strategy

Active Partial Rollout for handling long-tail generations:

```
Standard approach:     Wait for slowest â†’ GPU idle
                       â–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘

APRIL approach:        Oversample, abort long-tail, reuse partials
                       â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
```

## Quick Start

```python
from flux import FluxTrainer, FluxConfig

# Configure
config = FluxConfig(
    model_path="meta-llama/Llama-3-8B",
    sglang_url="http://localhost:8000",
    
    # Adaptive settings
    target_staleness=0.15,
    min_async_ratio=0.1,
    max_async_ratio=0.9,
)

# Train
trainer = FluxTrainer(config)
trainer.fit(
    prompts=train_prompts,
    num_steps=10000,
    eval_prompts=eval_prompts,
)
```

## Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     Adaptive Control Plane                       â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”‚
â”‚  â”‚ Adaptive  â”‚ â”‚  Smart    â”‚ â”‚ Staleness â”‚ â”‚Speculativeâ”‚       â”‚
â”‚  â”‚  Async    â”‚ â”‚  Batch    â”‚ â”‚  Monitor  â”‚ â”‚  Sync     â”‚       â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                   Lightweight Coordinator                        â”‚
â”‚              (asyncio + ZeroMQ, no Ray)                         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                   Native Execution Engines                       â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”            â”‚
â”‚  â”‚   Megatron-LM       â”‚â—„â”€â”€â–ºâ”‚      SGLang         â”‚            â”‚
â”‚  â”‚   (Training)        â”‚    â”‚    (Inference)      â”‚            â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜            â”‚
â”‚                    CUDA IPC Weight Sync                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## Performance

Preliminary targets (to be validated):

| Metric | VERL | AReaL | Flux |
|--------|------|-------|------|
| GPU Utilization | ~45% | ~95% | **~85%** |
| Training Stability | â˜…â˜…â˜…â˜…â˜… | â˜…â˜…â˜…â˜†â˜† | **â˜…â˜…â˜…â˜…â˜†** |
| Throughput | 1.0x | 1.8x | **2.0x** |

## Design Philosophy

### 1. "Continuous Spectrum, Not Binary Choice"

Every hyperparameter that could benefit from adaptation should be adaptive:
- Sync/async ratio
- Temperature
- Batch composition
- Compute allocation

### 2. "Native First"

Use the best existing tools directly:
- Don't wrap Megatron, integrate with it
- Don't wrap SGLang, call its HTTP API
- Don't use Ray, write simple Python

### 3. "Simple > Clever"

- < 5000 lines of core code
- No magic, explicit control flow
- Easy to debug and extend

## Comparison with Other Frameworks

| Aspect | VERL | AReaL | Slime | **Flux** |
|--------|------|-------|-------|----------|
| Sync Strategy | Fixed sync | Fixed async | Both | **Adaptive** |
| Orchestration | Ray | Custom | HTTP | **asyncio** |
| Training Backend | Megatron/FSDP | Custom | Megatron | **Megatron** |
| Inference Backend | vLLM/SGLang | Custom | SGLang | **SGLang** |
| Weight Sync | Ray Object Store | Custom | CUDA IPC | **CUDA IPC** |
| Staleness Handling | N/A (sync) | Staleness-aware PPO | APRIL | **Unified correction** |
| Code Complexity | Medium | High | Low | **Low** |

## Roadmap

- [x] Design specification
- [x] Core component skeleton
- [ ] Phase 1: Foundation (Megatron + SGLang integration)
- [ ] Phase 2: Adaptive components
- [ ] Phase 3: Optimizations
- [ ] Phase 4: Production readiness

## Contributing

Contributions welcome! See [CONTRIBUTING.md](CONTRIBUTING.md) for guidelines.

## Citation

```bibtex
@software{flux2025,
  author = {Xin},
  title = {Flux: An Adaptive Post-Training Framework for Large Language Models},
  year = {2025},
  url = {https://github.com/xxx/flux}
}
```

## License

Apache 2.0

---

<p align="center">
  <i>Flux: Where stability meets efficiency</i>
</p>
