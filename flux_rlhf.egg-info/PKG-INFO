Metadata-Version: 2.4
Name: flux-rlhf
Version: 0.1.0
Summary: An Adaptive Post-Training Framework for Large Language Models
Author-email: Flux Team <flux@example.com>
License: Apache-2.0
Project-URL: Homepage, https://github.com/flux-team/flux
Project-URL: Documentation, https://flux-rlhf.readthedocs.io
Project-URL: Repository, https://github.com/flux-team/flux
Project-URL: Issues, https://github.com/flux-team/flux/issues
Keywords: llm,rlhf,reinforcement-learning,post-training,machine-learning,deep-learning
Classifier: Development Status :: 3 - Alpha
Classifier: Intended Audience :: Developers
Classifier: Intended Audience :: Science/Research
Classifier: License :: OSI Approved :: Apache Software License
Classifier: Operating System :: OS Independent
Classifier: Programming Language :: Python :: 3
Classifier: Programming Language :: Python :: 3.10
Classifier: Programming Language :: Python :: 3.11
Classifier: Programming Language :: Python :: 3.12
Classifier: Topic :: Scientific/Engineering :: Artificial Intelligence
Requires-Python: >=3.10
Description-Content-Type: text/markdown
License-File: LICENSE
Requires-Dist: torch>=2.0.0
Requires-Dist: numpy>=1.24.0
Requires-Dist: aiohttp>=3.8.0
Requires-Dist: httpx>=0.24.0
Requires-Dist: pydantic>=2.0.0
Requires-Dist: PyYAML>=6.0
Requires-Dist: rich>=13.0.0
Requires-Dist: tqdm>=4.65.0
Requires-Dist: tensorboard>=2.13.0
Provides-Extra: dev
Requires-Dist: pytest>=7.3.0; extra == "dev"
Requires-Dist: pytest-asyncio>=0.21.0; extra == "dev"
Requires-Dist: pytest-cov>=4.1.0; extra == "dev"
Requires-Dist: pytest-mock>=3.11.0; extra == "dev"
Requires-Dist: mypy>=1.4.0; extra == "dev"
Requires-Dist: ruff>=0.0.280; extra == "dev"
Requires-Dist: black>=23.7.0; extra == "dev"
Requires-Dist: isort>=5.12.0; extra == "dev"
Requires-Dist: pre-commit>=3.3.0; extra == "dev"
Provides-Extra: sglang
Requires-Dist: sglang>=0.1.0; extra == "sglang"
Provides-Extra: megatron
Requires-Dist: megatron-core>=0.4.0; extra == "megatron"
Provides-Extra: all
Requires-Dist: flux-rlhf[dev,megatron,sglang]; extra == "all"
Dynamic: license-file

# Flux

```
███████╗██╗     ██╗   ██╗██╗  ██╗
██╔════╝██║     ██║   ██║╚██╗██╔╝
█████╗  ██║     ██║   ██║ ╚███╔╝
██╔══╝  ██║     ██║   ██║ ██╔██╗
██║     ███████╗╚██████╔╝██╔╝ ██╗
╚═╝     ╚══════╝ ╚═════╝ ╚═╝  ╚═╝
```

**Adaptive Post-Training Framework for LLMs**

[![License](https://img.shields.io/badge/License-Apache%202.0-blue.svg)](https://opensource.org/licenses/Apache-2.0)
[![Python 3.10+](https://img.shields.io/badge/python-3.10+-blue.svg)](https://www.python.org/downloads/)

> **The best of all worlds** — Synchronous stability + Asynchronous efficiency + Native simplicity

<p align="center">
  <a href="#installation">Installation</a> •
  <a href="#quick-start">Quick Start</a> •
  <a href="#key-features">Features</a> •
  <a href="#architecture">Architecture</a> •
  <a href="docs/flux-design-spec.md">Design Doc</a>
</p>

---

## Why Flux?

Existing RLHF frameworks force a binary choice between stability and throughput:

```
┌────────────────────────────────────────────────────────────────┐
│                                                                │
│   VERL        ████████████░░░░░░░░░░░░░░░░░░  Stable but slow │
│   AReaL       ░░░░░░░░░░░░░░░░░░████████████  Fast but risky  │
│                                                                │
│   Flux        ████████████████████████████░░  Best of both    │
│                                                                │
│               ◄── Sync ─────────────────── Async ──►          │
│                                                                │
│   Key Insight: Sync vs Async is NOT a binary choice           │
│   Flux adapts the ratio in real-time based on staleness       │
│                                                                │
└────────────────────────────────────────────────────────────────┘
```

| Aspect | VERL | AReaL | Slime | **Flux** |
|--------|------|-------|-------|----------|
| Sync Strategy | Fixed sync | Fixed async | Both | **Adaptive** |
| Orchestration | Ray | Custom | HTTP | **asyncio** |
| Training Backend | Megatron/FSDP | Custom | Megatron | **Megatron** |
| Inference Backend | vLLM/SGLang | Custom | SGLang | **SGLang** |
| Weight Sync | Ray Object Store | Custom | CUDA IPC | **CUDA IPC** |
| Staleness Handling | N/A | Staleness-aware PPO | APRIL | **Unified** |
| Code Complexity | ~15k LOC | ~25k LOC | ~8k LOC | **<5k LOC** |

---

## Installation

### Prerequisites

- Python 3.10+
- CUDA 12.0+ with NCCL
- [SGLang](https://github.com/sgl-project/sglang) server running
- [Megatron-LM](https://github.com/NVIDIA/Megatron-LM) (for distributed training)

### Install from source

```bash
git clone https://github.com/flux-team/flux.git
cd flux

# Basic installation
pip install -e .

# With development dependencies
pip install -e ".[dev]"

# Full installation (includes SGLang and Megatron dependencies)
pip install -e ".[all]"
```

---

## Quick Start

### 1. Start SGLang server

```bash
python -m sglang.launch_server --model-path Qwen/Qwen3-8B --port 8000
```

### 2. Run training

```python
from flux import FluxTrainer, FluxConfig, AdaptiveAsyncConfig, SGLangConfig

# Configure Flux
config = FluxConfig(
    model_path="Qwen/Qwen3-8B",

    # SGLang inference server
    sglang=SGLangConfig(base_url="http://localhost:8000"),

    # Adaptive async settings
    adaptive_async=AdaptiveAsyncConfig(
        target_staleness=0.15,  # Flux maintains this staleness level
        min_async_ratio=0.1,    # Never fully sync (wastes GPU)
        max_async_ratio=0.9,    # Never fully async (unstable)
    ),

    # Training settings
    learning_rate=1e-6,
    batch_size=32,
    num_steps=10000,
)

# Train
trainer = FluxTrainer(config)
trainer.fit(prompts, eval_prompts=eval_prompts)
```

### 3. Using YAML configuration

```bash
# Train with config file
flux train --config configs/qwen3-8b-8gpu.yaml --prompts data/prompts.jsonl
```

---

## Key Features

### Adaptive Async Control

Flux uses a PID controller to dynamically adjust the sync/async ratio based on measured staleness:

```
                    Staleness Measurement
                            │
            ┌───────────────┼───────────────┐
            │               │               │
        KL Divergence   IW Variance    Version Gap
            │               │               │
            └───────────────┼───────────────┘
                            │
                            ▼
                    ┌───────────────┐
                    │ PID Controller │
                    │   target=0.15  │
                    └───────┬───────┘
                            │
                            ▼
                     Async Ratio
                   (0.1 → 0.9)
```

- **Early training**: More sync (policy changing rapidly)
- **Late training**: More async (policy stable, maximize throughput)

### APRIL Strategy

Active Partial Rollout for efficient generation:

1. **Oversample**: Generate 1.5x prompts to have buffer
2. **Abort**: Cancel long-tail generations after timeout
3. **Reuse**: Save partial trajectories for continuation

### Smart Batch Composition

Optimized training batches with:
- **Length bucketing**: Group similar lengths to minimize padding
- **Staleness balancing**: Stratified sampling to reduce importance weight variance
- **Curriculum ordering**: Easy → hard progression as training proceeds

### Zero-Copy Weight Sync

CUDA IPC for same-node weight transfer between Megatron and SGLang:
- No serialization overhead
- Delta compression for incremental updates
- Lazy sync (only when inference needs fresh weights)

---

## Architecture

```
┌─────────────────────────────────────────────────────────────────┐
│                        FLUX ARCHITECTURE                        │
├─────────────────────────────────────────────────────────────────┤
│                                                                 │
│  ┌───────────────────────────────────────────────────────────┐  │
│  │           Layer 3: Adaptive Control Plane                 │  │
│  │                                                           │  │
│  │  ┌─────────────┐ ┌─────────────┐ ┌─────────────┐         │  │
│  │  │  Adaptive   │ │   Smart     │ │  Staleness  │         │  │
│  │  │   Async     │ │   Batch     │ │   Monitor   │         │  │
│  │  │ Controller  │ │  Composer   │ │             │         │  │
│  │  └──────┬──────┘ └──────┬──────┘ └──────┬──────┘         │  │
│  └─────────┼───────────────┼───────────────┼─────────────────┘  │
│            └───────────────┼───────────────┘                    │
│                            │                                    │
│  ┌─────────────────────────▼─────────────────────────────────┐  │
│  │          Layer 2: Lightweight Coordinator                 │  │
│  │                  (asyncio + ZeroMQ)                       │  │
│  │                                                           │  │
│  │  • Async event loop for non-blocking orchestration        │  │
│  │  • Weight sync scheduling                                 │  │
│  │  • Checkpoint management                                  │  │
│  └─────────────────────────┬─────────────────────────────────┘  │
│                            │                                    │
│            ┌───────────────┴───────────────┐                    │
│            │                               │                    │
│  ┌─────────▼─────────┐           ┌─────────▼─────────┐          │
│  │   Layer 1a:       │           │   Layer 1b:       │          │
│  │   Megatron-LM     │◄─────────►│   SGLang          │          │
│  │   (Training)      │ CUDA IPC  │   (Inference)     │          │
│  │                   │ Zero-Copy │                   │          │
│  │  • 3D Parallelism │           │  • Continuous     │          │
│  │  • Mixed Precision│           │    Batching       │          │
│  │  • Grad Ckpt      │           │  • RadixAttention │          │
│  └───────────────────┘           └───────────────────┘          │
│                                                                 │
└─────────────────────────────────────────────────────────────────┘
```

### Training Step Flow

```
1. Coordinator requests rollouts → SGLang generates (streaming)
                                          │
2. As responses complete ─────────────────┴──► Data Buffer
                                                    │
3. Adaptive Controller monitors staleness ◄─────────┤
   │                                                │
   ├─ Staleness LOW:  continue async, request more  │
   └─ Staleness HIGH: trigger sync barrier          │
                                                    │
4. Smart Batch Composer creates training batch ◄────┘
   │
   ├─► Group by length (reduce padding)
   ├─► Balance staleness (stratified sampling)
   └─► Curriculum ordering (easy → hard)
                │
5. Megatron executes training step
   │
   ├─► Importance-corrected loss
   ├─► Gradient accumulation + sync
   └─► Optimizer step
                │
6. Weight Sync Manager updates SGLang (lazy, delta-compressed)
```

---

## Project Structure

```
flux/
├── core/                    # Core abstractions
│   ├── config.py           # Pydantic configuration classes
│   ├── types.py            # Type definitions (AsyncDecision, BatchMetrics, etc.)
│   └── trajectory.py       # Trajectory data structures
│
├── controller/              # Adaptive control plane
│   ├── adaptive_async.py   # PID-based async ratio controller
│   ├── staleness.py        # Staleness measurement (KL, IW variance, version gap)
│   └── importance.py       # Unified importance weight correction
│
├── rollout/                 # Rollout generation
│   ├── manager.py          # Streaming rollout with APRIL strategy
│   ├── sglang_client.py    # SGLang HTTP client
│   └── length_predictor.py # Output length prediction for scheduling
│
├── training/                # Training engine
│   ├── megatron_engine.py  # Megatron-LM integration
│   ├── batch_composer.py   # Smart batch composition
│   └── algorithms/         # PPO, GRPO, etc.
│
├── sync/                    # Weight synchronization
│   ├── weight_sync.py      # Sync manager with lazy updates
│   ├── cuda_ipc.py         # Zero-copy CUDA IPC transfer
│   └── delta_compression.py # Sparse delta encoding
│
└── coordinator/             # Lightweight coordinator
    ├── coordinator.py      # Main asyncio coordinator
    └── checkpoint.py       # Checkpoint management
```

---

## Benchmarks

Target performance on Qwen3-8B with 8x H100:

```
                    VERL        AReaL       FLUX
                    ────        ─────       ────
GPU Utilization     ~45%        ~95%        ~85%
                    ▓▓▓░░░░░░░  ▓▓▓▓▓▓▓▓▓░  ▓▓▓▓▓▓▓▓░░

Stability           ★★★★★       ★★★☆☆       ★★★★☆
                    Excellent   Risky       Good

Throughput          1.0x        1.8x        2.0x
                    ▓▓▓▓░░░░░░  ▓▓▓▓▓▓▓░░░  ▓▓▓▓▓▓▓▓░░
```

---

## Design Philosophy

### 1. Continuous Spectrum, Not Binary Choice

```
Sync ◄──────────────────────────────────────► Async
     │                                        │
     │    Flux operates anywhere on this      │
     │    spectrum, adapting in real-time     │
     │                                        │
     └────────────────────────────────────────┘
```

### 2. Native First

- **Don't wrap Megatron** → Integrate with Megatron-LM directly
- **Don't wrap SGLang** → Use SGLang's HTTP API
- **Don't use Ray** → Write simple asyncio Python

### 3. Simple > Clever

- < 5000 lines of core code
- No magic, explicit control flow
- Easy to debug and extend

---

## Why Not Ray?

| Ray Provides | LLM Training Needs | Mismatch |
|--------------|-------------------|----------|
| Task scheduling | NCCL collectives | Ray doesn't understand NCCL topology |
| Actor management | Fine-grained GPU memory | Ray treats GPU as resource count |
| Flexible placement | Fixed TP/PP/DP config | Changing parallelism requires restart |
| Object Store | CUDA IPC / NCCL | Ray serialization is slow for large tensors |

Both VERL and Slime bypass Ray for critical paths. If you're bypassing the framework, why use it?

---

## Documentation

- [Design Specification](docs/flux-design-spec.md) - Detailed design document
- [Project Structure](docs/flux-project-structure.md) - Implementation status
- [API Reference](docs/) - API documentation (coming soon)

---

## Contributing

We welcome contributions! Please see our contributing guidelines.

```bash
# Setup development environment
pip install -e ".[dev]"

# Run tests
pytest

# Code quality checks
ruff check . && black --check . && mypy flux/
```

---

## Citation

```bibtex
@software{flux2025,
  title = {Flux: An Adaptive Post-Training Framework for LLMs},
  year = {2025},
  url = {https://github.com/flux-team/flux}
}
```

## License

Apache 2.0

---

<p align="center">
  <b>Flux: Where stability meets efficiency</b>
</p>
