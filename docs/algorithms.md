# Flux Algorithm Guide

Flux supports multiple reinforcement learning algorithms for LLM post-training. This guide explains each algorithm and when to use them.

## Overview

All algorithms in Flux follow a common pattern:

1. **Advantage Estimation**: Compute how much better an action was than expected
2. **Policy Loss**: Compute the loss to update the policy
3. **Importance Correction**: Adjust for off-policy data (staleness)

## Built-in Algorithms

### PPO (Proximal Policy Optimization)

The classic RL algorithm for LLM training. Uses clipped surrogate objective to prevent large policy updates.

**When to use**: Default choice for stable training with moderate performance.

```yaml
algorithm:
  name: "ppo"
  clip_ratio: 0.2
  kl_penalty: 0.1
  target_kl: 0.01
```

**Key parameters**:
- `clip_ratio`: Clipping range for policy ratio (0.1-0.3)
- `kl_penalty`: KL divergence penalty coefficient
- `target_kl`: Target KL divergence for early stopping

**Loss function**:
```
L = -min(ratio * A, clip(ratio, 1-ε, 1+ε) * A) - β * H(π)
```

### GRPO (Group Relative Policy Optimization)

Groups multiple responses per prompt and uses relative rankings for advantage estimation. Default in Flux.

**When to use**: When generating multiple responses per prompt. More sample-efficient than PPO.

```yaml
algorithm:
  name: "grpo"
  group_size: 4
  baseline: "mean"
```

**Key parameters**:
- `group_size`: Number of responses per prompt
- `baseline`: How to compute baseline ("mean", "leave_one_out")

**Advantage estimation**:
```
A_i = (r_i - mean(r_group)) / std(r_group)
```

### DPO (Direct Preference Optimization)

Directly optimizes preferences without explicit reward model. Works with preference pairs.

**When to use**: When you have preference data (chosen vs rejected).

```yaml
algorithm:
  name: "dpo"
  beta: 0.1
  reference_free: false
```

**Key parameters**:
- `beta`: Temperature parameter (0.01-0.5)
- `reference_free`: Skip reference model computation

**Loss function**:
```
L = -log(σ(β * (log π(y_w|x)/π_ref(y_w|x) - log π(y_l|x)/π_ref(y_l|x))))
```

### REINFORCE

Basic policy gradient with reward-weighted log probabilities.

**When to use**: Simple baseline or when other methods are unstable.

```yaml
algorithm:
  name: "reinforce"
  baseline: "moving_average"
  baseline_decay: 0.99
```

**Key parameters**:
- `baseline`: Baseline type ("none", "moving_average", "learned")
- `baseline_decay`: Decay for moving average baseline

**Loss function**:
```
L = -log π(a|s) * (R - b)
```

### DAPO (Decoupled Clip and Dynamic Sampling)

Advanced PPO variant with separate clipping for positive/negative advantages and dynamic sample selection.

**When to use**: When PPO shows instability or when working with high-variance rewards.

```yaml
algorithm:
  name: "dapo"
  clip_ratio_low: 0.2
  clip_ratio_high: 0.28
  dynamic_sampling: true
  token_level_loss: true
```

**Key parameters**:
- `clip_ratio_low`: Clip for negative advantages
- `clip_ratio_high`: Clip for positive advantages
- `dynamic_sampling`: Enable dynamic sample weighting
- `token_level_loss`: Per-token loss computation

### RLOO (REINFORCE Leave-One-Out)

Uses leave-one-out baseline for variance reduction with multiple samples.

**When to use**: When you can afford multiple samples per prompt and want lower variance.

```yaml
algorithm:
  name: "rloo"
  num_samples: 4
```

**Advantage**:
```
A_i = r_i - (1/(n-1)) * sum(r_j for j != i)
```

### GSPO (Group Stability Policy Optimization)

Combines GRPO with stability improvements for distributed training.

**When to use**: Large-scale distributed training where consistency matters.

```yaml
algorithm:
  name: "gspo"
  group_size: 4
  stability_coef: 0.1
```

## Importance Correction

When using asynchronous training, data may be generated by older policy versions. Flux uses importance weighting to correct for this:

```python
# Importance weight
w = π_current(a|s) / π_old(a|s)

# Clipped importance weight (prevents extreme values)
w_clipped = clip(w, 0.5, 2.0)

# Loss with importance correction
L = w_clipped * L_original
```

### Staleness Metrics

Flux tracks multiple staleness indicators:

1. **Version Gap**: `current_version - trajectory_version`
2. **KL Divergence**: `KL(π_current || π_old)`
3. **Importance Weight Variance**: `var(w)`

The adaptive async controller uses these to adjust the sync/async ratio.

## Adding Custom Algorithms

Flux uses a registry pattern for easy extension:

### Custom Advantage Estimator

```python
from flux.training.algorithms.base import register_adv_estimator
import torch

@register_adv_estimator("my_advantage")
def compute_my_advantage(
    rewards: torch.Tensor,
    mask: torch.Tensor,
    gamma: float = 1.0,
    **kwargs,
) -> tuple[torch.Tensor, torch.Tensor]:
    """Custom advantage computation.

    Args:
        rewards: Reward tensor [batch, seq_len]
        mask: Valid token mask [batch, seq_len]
        gamma: Discount factor

    Returns:
        advantages: Advantage estimates
        returns: Return estimates
    """
    # Your implementation
    advantages = rewards - rewards.mean()
    returns = rewards
    return advantages, returns
```

### Custom Policy Loss

```python
from flux.training.algorithms.base import register_policy_loss
import torch

@register_policy_loss("my_loss")
def compute_my_loss(
    old_logp: torch.Tensor,
    logp: torch.Tensor,
    advantages: torch.Tensor,
    mask: torch.Tensor,
    **kwargs,
) -> tuple[torch.Tensor, dict]:
    """Custom policy loss.

    Args:
        old_logp: Log probs from behavior policy
        logp: Log probs from current policy
        advantages: Advantage estimates
        mask: Valid token mask

    Returns:
        loss: Scalar loss
        metrics: Dictionary of metrics
    """
    ratio = torch.exp(logp - old_logp)
    loss = -(ratio * advantages * mask).sum() / mask.sum()

    metrics = {
        "mean_ratio": ratio.mean().item(),
        "loss": loss.item(),
    }

    return loss, metrics
```

### Using Custom Algorithm

```yaml
algorithm:
  name: "my_loss"
  advantage_estimator: "my_advantage"
  custom_param: 0.5
```

Or in Python:

```python
from flux import FluxConfig
from flux.training.algorithms import PolicyGradientOptimizer

config = FluxConfig(
    algorithm={"name": "my_loss", "advantage_estimator": "my_advantage"},
)

optimizer = PolicyGradientOptimizer(
    algorithm="my_loss",
    advantage_estimator="my_advantage",
)
```

## Algorithm Comparison

| Algorithm | Stability | Sample Efficiency | Compute Cost | Best For |
|-----------|-----------|-------------------|--------------|----------|
| PPO | High | Medium | Medium | General use |
| GRPO | High | High | Low | Multi-sample |
| DPO | Medium | High | Low | Preference data |
| REINFORCE | Low | Low | Low | Simple baselines |
| DAPO | Very High | Medium | High | Difficult tasks |
| RLOO | Medium | High | Medium | Variance reduction |
| GSPO | Very High | High | Medium | Distributed |

## Hyperparameter Tuning

### Learning Rate

- Start with `1e-6` to `1e-5`
- Lower for larger models
- Use warmup for stability

### Clip Ratio (PPO/DAPO)

- `0.1`: Conservative, stable
- `0.2`: Standard, balanced
- `0.3`: Aggressive, faster learning

### Group Size (GRPO/RLOO)

- `4`: Balanced cost/benefit
- `8`: Better estimates, higher cost
- `16`: Best estimates, very expensive

### Temperature

- `0.7-0.9`: Moderate diversity
- `1.0`: Standard sampling
- `1.2+`: High diversity (may reduce quality)

## Debugging Tips

### Loss Not Decreasing

1. Check reward scale (normalize to [-1, 1])
2. Reduce learning rate
3. Increase clip ratio
4. Check for NaN in log probs

### Training Unstable

1. Add KL penalty
2. Use smaller batches
3. Enable gradient clipping
4. Reduce async ratio

### Poor Sample Efficiency

1. Try GRPO or RLOO
2. Increase group size
3. Use curriculum learning
4. Check reward function quality

## Next Steps

- [API Documentation](api.md) - Full API reference
- [Examples](../examples/) - Working code examples
- [Configuration](configuration.md) - All config options
